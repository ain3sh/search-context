name: Sync Context Search Stores

on:
  # Run daily at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Allow manual trigger for testing or immediate updates
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-stores:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout search-context repository (for workflow & state file)
        uses: actions/checkout@v4
        with:
          path: search-context
          fetch-depth: 0

      - name: Checkout ain3sh/docs repository (for indexing)
        uses: actions/checkout@v4
        with:
          repository: ain3sh/docs
          path: docs
          fetch-depth: 0  # Need full history for change detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install google-genai python-dotenv

      - name: Get last sync timestamp (from state file in search-context)
        id: last-sync
        run: |
          STATE_FILE="search-context/.context-sync/last-docs-commit.txt"
          if [ -f "$STATE_FILE" ]; then
            LAST_SYNC_COMMIT=$(cat "$STATE_FILE")
            echo "Found previous sync marker: $LAST_SYNC_COMMIT"
            echo "last_commit=$LAST_SYNC_COMMIT" >> $GITHUB_OUTPUT
          else
            echo "No previous sync marker file found - full sync will run."
            echo "last_commit=" >> $GITHUB_OUTPUT
          fi

      - name: Detect changed directories since last sync
        id: changed-dirs
        env:
          LAST_SYNC_COMMIT: ${{ steps.last-sync.outputs.last_commit }}
        run: |
          cd docs
          CHANGED_DIRS=$(python - <<'EOF'
          import os
          import sys
          import json
          from pathlib import Path

          def get_indexable_stores(docs_path, max_depth=2):
              """
              Find all indexable documentation directories.
              Only includes directories with files at their root level.
              Skips empty namespace containers.
              """
              indexable = []

              def scan_directory(path, depth=0, prefix=""):
                  if depth > max_depth:
                      return

                  files_at_root = sum(
                      1 for f in path.iterdir()
                      if f.is_file() and not f.name.startswith('.')
                  )

                  if files_at_root > 0:
                      store_name = f"{prefix}{path.name}" if prefix else path.name
                      indexable.append(store_name)
                      print(f"  ‚úÖ Indexable: {store_name} ({files_at_root} files at root)", file=sys.stderr)
                  else:
                      print(f"  ‚è≠Ô∏è  Skipping: {prefix}{path.name} (0 files at root, checking subdirs)", file=sys.stderr)
                      subdirs = [
                          d for d in path.iterdir()
                          if d.is_dir() and not d.name.startswith('.')
                      ]
                      for subdir in subdirs:
                          new_prefix = f"{prefix}{path.name}/" if prefix or depth > 0 else f"{path.name}/"
                          scan_directory(subdir, depth + 1, new_prefix)

              for item in Path(docs_path).iterdir():
                  if item.is_dir() and not item.name.startswith('.'):
                      scan_directory(item, depth=0)

              return indexable

          last_commit = os.environ.get('LAST_SYNC_COMMIT', '')

          if not last_commit:
              print("First (or full) sync - scanning all directories", file=sys.stderr)
              all_stores = get_indexable_stores('.')
              print(json.dumps(all_stores))
          else:
              import subprocess
              result = subprocess.run(
                  ['git', 'diff', '--name-only', last_commit, 'HEAD'],
                  capture_output=True,
                  text=True
              )
              changed_files = result.stdout.strip().split('\n')

              if not changed_files or changed_files == ['']:
                  print("No changes since last sync", file=sys.stderr)
                  print(json.dumps([]))
              else:
                  changed_paths = set()
                  for file in changed_files:
                      if file.startswith('.github/'):
                          continue
                      parts = file.split('/')
                      if parts:
                          changed_paths.add(parts[0])
                      if len(parts) >= 2:
                          changed_paths.add(f"{parts[0]}/{parts[1]}")

                  all_stores = get_indexable_stores('.')
                  changed_stores = [s for s in all_stores if s in changed_paths]

                  print(f"Changed paths considered: {len(changed_paths)}", file=sys.stderr)
                  print(f"Filtered to indexable changed stores: {changed_stores}", file=sys.stderr)
                  print(json.dumps(changed_stores))
          EOF
          )
          {
            printf '%s\n' "changed_dirs<<EOF_CHANGED_DIRS"
            printf '%s\n' "$CHANGED_DIRS"
            printf '%s\n' "EOF_CHANGED_DIRS"
          } >> $GITHUB_OUTPUT

      - name: Sync FileSearchStores
        id: sync
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CHANGED_DIRS: ${{ steps.changed-dirs.outputs.changed_dirs }}
        run: |
          python - <<'EOF'
          import os
          import json
          import time
          from pathlib import Path
          from google import genai

          client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
          changed_dirs = json.loads(os.environ.get('CHANGED_DIRS', '[]'))

          def write_output(key, value):
              out = os.environ.get('GITHUB_OUTPUT')
              if out:
                  with open(out, 'a') as f:
                      f.write(f"{key}={value}\n")

          if not changed_dirs:
              print("No directories changed, skipping sync.")
              write_output("synced_count", 0)
              # Provide an empty summary file for later step
              with open('/tmp/sync_summary.txt', 'w') as f:
                  f.write("synced_count=0\n")
                  f.write("total_cost=0.0000\n")
              exit(0)

          total_cost = 0.0

          for store_name in changed_dirs:
              print(f"\n{'='*60}")
              print(f"Processing store: {store_name}")
              print(f"{'='*60}")

              # Delete existing store if present
              try:
                  stores = list(client.file_search_stores.list())
                  existing = [s for s in stores if s.display_name == store_name]
                  if existing:
                      print(f"Deleting existing store: {existing[0].name}")
                      client.file_search_stores.delete(name=existing[0].name)
                      time.sleep(2)
              except Exception as e:
                  print(f"‚ùå Error deleting store {store_name}: {e}")
                  print("   Skipping deletion and continuing with creation attempt.")

              # Create new store
              print(f"Creating new FileSearchStore: {store_name}")
              store = client.file_search_stores.create(config={'display_name': store_name})

              store_path = Path('docs') / store_name
              files = (
                  list(store_path.rglob('*.md')) +
                  list(store_path.rglob('*.txt')) +
                  list(store_path.rglob('*.py')) +
                  list(store_path.rglob('*.js')) +
                  list(store_path.rglob('*.json'))
              )

              print(f"Found {len(files)} files to upload.")
              operations = []
              total_tokens = 0

              for file_path in files:
                  try:
                      file_size = file_path.stat().st_size
                      estimated_tokens = file_size // 4
                      total_tokens += estimated_tokens

                      print(f"Uploading: {file_path}")
                      op = client.file_search_stores.upload_to_file_search_store(
                          file=str(file_path),
                          file_search_store_name=store.name,
                          config={'display_name': str(file_path.relative_to(store_path))}
                      )
                      operations.append(op)
                  except Exception as e:
                      print(f"Error uploading {file_path}: {e}")

              print("Waiting for uploads to complete...")
              for i, op in enumerate(operations):
                  max_retries = 120
                  retries = 0
                  while not op.done:
                      if retries >= max_retries:
                          raise TimeoutError("Upload operation timed out")
                      time.sleep(5)
                      op = client.operations.get(op)
                      retries += 1
                  print(f"  Upload {i+1}/{len(operations)} complete")

              cost = (total_tokens / 1_000_000) * 0.15
              total_cost += cost

              print(f"\nStore '{store_name}' sync complete:")
              print(f"  Files indexed: {len(files)}")
              print(f"  Estimated tokens: {total_tokens:,}")
              print(f"  Cost: ${cost:.4f}")

          print(f"\n{'='*60}")
          print(f"Total sync cost: ${total_cost:.4f}")
          print(f"Directories synced: {len(changed_dirs)}")
          print(f"{'='*60}")

          with open('/tmp/sync_summary.txt', 'w') as f:
              f.write(f"synced_count={len(changed_dirs)}\n")
              f.write(f"total_cost={total_cost:.4f}\n")

          write_output("synced_count", len(changed_dirs))
          EOF

      - name: Record last synced docs commit (state file in search-context)
        if: steps.sync.outcome == 'success' && steps.sync.outputs.synced_count != '0'
        run: |
          DOCS_SHA=$(cd docs && git rev-parse HEAD)
          mkdir -p search-context/.context-sync
          echo "$DOCS_SHA" > search-context/.context-sync/last-docs-commit.txt
          cd search-context
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          if git diff --quiet .context-sync/last-docs-commit.txt; then
            echo "State file unchanged; no commit needed."
          else
            git add .context-sync/last-docs-commit.txt
            git commit -m "chore: update last synced docs commit to $DOCS_SHA"
            git push origin HEAD:main
            echo "Recorded and pushed docs commit $DOCS_SHA"
          fi

      - name: Report sync status
        if: always()
        run: |
          if [ -f /tmp/sync_summary.txt ]; then
            source /tmp/sync_summary.txt
            echo "üìä Sync Summary:"
            echo "   Directories synced: $synced_count"
            echo "   Total cost: \$$total_cost"
            echo "   State file path: search-context/.context-sync/last-docs-commit.txt"
            echo "   Next scheduled sync: Tomorrow at 2 AM UTC"
          else
            echo "‚ö†Ô∏è Sync did not complete successfully"
