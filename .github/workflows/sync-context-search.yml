name: Sync Context Search Stores

on:
  # Run daily at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Allow manual trigger for testing or immediate updates
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-stores:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout search-context repository (for workflow & state file)
        uses: actions/checkout@v4
        with:
          path: search-context
          fetch-depth: 0

      - name: Checkout ain3sh/docs repository (for indexing)
        uses: actions/checkout@v4
        with:
          repository: ain3sh/docs
          path: docs
          fetch-depth: 0  # Need full history for change detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install google-genai python-dotenv

      - name: Get last sync timestamp (from state file in search-context)
        id: last-sync
        shell: bash
        run: |
          set -euo pipefail
          STATE_FILE="search-context/.context-sync/last-docs-commit.txt"
          if [ -f "$STATE_FILE" ]; then
            LAST_SYNC_COMMIT=$(cat "$STATE_FILE")
            echo "Found previous sync marker: $LAST_SYNC_COMMIT"
            echo "last_commit=$LAST_SYNC_COMMIT" >> $GITHUB_OUTPUT
          else
            echo "No previous sync marker file found - full sync will run."
            echo "last_commit=" >> $GITHUB_OUTPUT
          fi

      - name: Detect changed directories since last sync
        id: changed-dirs
        env:
          LAST_SYNC_COMMIT: ${{ steps.last-sync.outputs.last_commit }}
        shell: bash
        run: |
          set -euo pipefail
          cd docs
          CHANGED_DIRS=$(python - <<'EOF'
          import os
          import sys
          import json
          from pathlib import Path

          def get_indexable_stores(docs_path, max_depth=2):
              """
              Find all indexable documentation directories.
              Only includes directories with files at their root level.
              Skips empty namespace containers.
              """
              indexable = []

              def scan_directory(path, depth=0, prefix=""):
                  if depth > max_depth:
                      return

                  files_at_root = sum(
                      1 for f in path.iterdir()
                      if f.is_file() and not f.name.startswith('.')
                  )

                  if files_at_root > 0:
                      store_name = f"{prefix}{path.name}" if prefix else path.name
                      indexable.append(store_name)
                      print(f"  ‚úÖ Indexable: {store_name} ({files_at_root} files at root)", file=sys.stderr)
                  else:
                      print(f"  ‚è≠Ô∏è  Skipping: {prefix}{path.name} (0 files at root, checking subdirs)", file=sys.stderr)
                      subdirs = [
                          d for d in path.iterdir()
                          if d.is_dir() and not d.name.startswith('.')
                      ]
                      for subdir in subdirs:
                          new_prefix = f"{prefix}{path.name}/" if prefix or depth > 0 else f"{path.name}/"
                          scan_directory(subdir, depth + 1, new_prefix)

              for item in Path(docs_path).iterdir():
                  if item.is_dir() and not item.name.startswith('.'):
                      scan_directory(item, depth=0)

              return indexable

          last_commit = os.environ.get('LAST_SYNC_COMMIT', '')

          if not last_commit:
              print("First (or full) sync - scanning all directories", file=sys.stderr)
              all_stores = get_indexable_stores('.')
              print(json.dumps(all_stores))
          else:
              import subprocess
              result = subprocess.run(
                  ['git', 'diff', '--name-only', last_commit, 'HEAD'],
                  capture_output=True,
                  text=True
              )
              changed_files = result.stdout.strip().split('\n')

              if not changed_files or changed_files == ['']:
                  print("No changes since last sync", file=sys.stderr)
                  print(json.dumps([]))
              else:
                  changed_paths = set()
                  for file in changed_files:
                      if file.startswith('.github/'):
                          continue
                      parts = file.split('/')
                      if parts:
                          changed_paths.add(parts[0])
                      if len(parts) >= 2:
                          changed_paths.add(f"{parts[0]}/{parts[1]}")

                  all_stores = get_indexable_stores('.')
                  changed_stores = [s for s in all_stores if s in changed_paths]

                  print(f"Changed paths considered: {len(changed_paths)}", file=sys.stderr)
                  print(f"Filtered to indexable changed stores: {changed_stores}", file=sys.stderr)
                  print(json.dumps(changed_stores))
          EOF
          )
          {
            printf '%s\n' "changed_dirs<<EOF_CHANGED_DIRS"
            printf '%s\n' "$CHANGED_DIRS"
            printf '%s\n' "EOF_CHANGED_DIRS"
          } >> $GITHUB_OUTPUT

      - name: Sync FileSearchStores
        id: sync
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CHANGED_DIRS: ${{ steps.changed-dirs.outputs.changed_dirs }}
        shell: bash
        run: |
          set -euo pipefail
          python - <<'EOF'
          import os
          import json
          import time
          import tempfile
          import shutil
          from pathlib import Path
          from google import genai
          from concurrent.futures import ThreadPoolExecutor, as_completed

          # Minimum file size in bytes (skip empty files)
          MIN_FILE_SIZE = 10

          client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
          changed_dirs = json.loads(os.environ.get('CHANGED_DIRS', '[]'))

          def write_output(key, value):
              out = os.environ.get('GITHUB_OUTPUT')
              if out:
                  with open(out, 'a') as f:
                      f.write(f"{key}={value}\n")

          def should_process_file(file_path):
              """Determine if a file should be processed based on extension and size."""
              # Supported extensions (including MDX which we'll handle specially)
              extensions = {'.md', '.mdx', '.txt', '.py', '.js', '.json', '.ts', '.tsx', '.jsx'}

              # Check extension
              if file_path.suffix.lower() not in extensions:
                  return False

              # Check file size
              try:
                  file_size = file_path.stat().st_size
                  if file_size < MIN_FILE_SIZE:
                      return False

                  # Special handling for __init__.py files
                  if file_path.name == '__init__.py' and file_size < 100:
                      return False
              except Exception:
                  return False

              return True

          def prepare_file_for_upload(file_path, temp_dir):
              """
              Prepare a file for upload. For MDX files, create a copy with .md extension.
              Returns tuple of (upload_path, display_name).
              """
              if file_path.suffix.lower() == '.mdx':
                  # Create a temporary copy with .md extension
                  temp_name = file_path.stem + '.md'
                  temp_path = Path(temp_dir) / temp_name
                  shutil.copy2(file_path, temp_path)
                  # Keep original name for display
                  return temp_path, file_path.name
              else:
                  # Use original file as-is
                  return file_path, file_path.name

          if not changed_dirs:
              print("No directories changed, skipping sync.")
              write_output("synced_count", 0)
              with open('/tmp/sync_summary.txt', 'w') as f:
                  f.write("synced_count=0\n")
                  f.write("total_cost=0.0000\n")
              exit(0)

          total_cost = 0.0
          total_files_uploaded = 0
          total_files_skipped = 0
          total_files_failed = 0

          for store_name in changed_dirs:
              print(f"\n{'='*60}")
              print(f"Processing store: {store_name}")
              print(f"{'='*60}")

              # Delete existing store if present
              try:
                  stores = list(client.file_search_stores.list())
                  existing = [s for s in stores if s.display_name == store_name]
                  if existing:
                      print(f"Deleting existing store: {existing[0].name}")
                      client.file_search_stores.delete(
                          name=existing[0].name,
                          config={'force': True}
                      )
                      time.sleep(2)
              except Exception as e:
                  print(f"‚ùå Error deleting store {store_name}: {e}")
                  print("   Continuing with creation attempt.")

              # Create new store
              print(f"Creating new FileSearchStore: {store_name}")
              try:
                  store = client.file_search_stores.create(config={'display_name': store_name})
              except Exception as e:
                  print(f"‚ùå Failed to create store: {e}")
                  continue

              # Collect files with improved filtering
              store_path = Path('docs') / store_name
              files = []
              skipped_count = 0

              for root, _, filenames in os.walk(store_path):
                  for filename in filenames:
                      file_path = Path(root) / filename
                      if should_process_file(file_path):
                          files.append(file_path)
                      else:
                          skipped_count += 1

              total_files_skipped += skipped_count
              print(f"Found {len(files)} files to upload (skipped {skipped_count} files).")

              if len(files) == 0:
                  print("‚ö†Ô∏è  No files found after filtering, skipping store.")
                  continue

              # Create temporary directory for MDX file conversion
              with tempfile.TemporaryDirectory() as temp_dir:
                  # Upload files with improved error handling
                  operations = []
                  total_tokens = 0
                  failed_uploads = []

                  def upload_file(file_path):
                      """Upload a single file and return operation"""
                      try:
                          # Prepare file (convert MDX if needed)
                          upload_path, display_name = prepare_file_for_upload(file_path, temp_dir)

                          file_size = upload_path.stat().st_size
                          estimated_tokens = file_size // 4

                          # Upload using the prepared path
                          op = client.file_search_stores.upload_to_file_search_store(
                              file=str(upload_path),
                              file_search_store_name=store.name,
                              config={'display_name': display_name}
                          )

                          return (op, estimated_tokens, file_path, None)
                      except Exception as e:
                          error_msg = str(e)
                          return (None, 0, file_path, error_msg)

                  # Upload files in parallel (max 10 concurrent)
                  print("Uploading files in parallel...")
                  with ThreadPoolExecutor(max_workers=10) as executor:
                      futures = {executor.submit(upload_file, f): f for f in files}

                      for i, future in enumerate(as_completed(futures), 1):
                          op, tokens, file_path, error = future.result()
                          if error:
                              total_files_failed += 1
                              failed_uploads.append((file_path.name, error))
                              # Only show first few errors inline
                              if total_files_failed <= 5:
                                  print(f"  ‚ùå [{i}/{len(files)}] Error: {file_path.name}: {error}")
                          else:
                              operations.append(op)
                              total_tokens += tokens
                              total_files_uploaded += 1
                              if i % 10 == 0 or i == len(files):
                                  print(f"  ‚úÖ [{i}/{len(files)}] queued")

              # Wait for uploads to complete
              if operations:
                  print(f"Waiting for {len(operations)} uploads to complete...")
                  completed = 0
                  max_wait = 600  # 10 minutes max
                  start_time = time.time()
                  last_report = 0

                  while completed < len(operations):
                      if time.time() - start_time > max_wait:
                          print(f"‚ö†Ô∏è  Timeout: {completed}/{len(operations)} completed")
                          break

                      # Check all operations in batch
                      newly_completed = 0
                      for i, op in enumerate(operations):
                          if not op.done:
                              try:
                                  operations[i] = client.operations.get(op)
                                  if operations[i].done:
                                      newly_completed += 1
                              except Exception:
                                  pass  # Ignore errors in status checks

                      completed += newly_completed

                      # Report progress every 10 completions or when done
                      if newly_completed > 0 and (completed - last_report >= 10 or completed == len(operations)):
                          print(f"  [{completed}/{len(operations)}] completed")
                          last_report = completed

                      if completed < len(operations):
                          time.sleep(2)  # Poll every 2s

              cost = (total_tokens / 1_000_000) * 0.15
              total_cost += cost

              print(f"\n‚úÖ Store '{store_name}' sync complete:")
              print(f"   Files indexed: {len(operations)}")
              print(f"   Files skipped: {skipped_count}")
              if failed_uploads:
                  print(f"   Files failed: {len(failed_uploads)}")
                  # Show MDX conversion note if any MDX files were processed
                  mdx_count = sum(1 for f in files if f.suffix.lower() == '.mdx')
                  if mdx_count > 0:
                      print(f"   MDX files converted: {mdx_count}")
              print(f"   Estimated tokens: {total_tokens:,}")
              print(f"   Cost: ${cost:.4f}")

              # Show summary of failures if there were many
              if len(failed_uploads) > 5:
                  print(f"\n‚ö†Ô∏è  Total of {len(failed_uploads)} files failed to upload")

          print(f"\n{'='*60}")
          print(f"‚úÖ Total sync summary:")
          print(f"   Directories synced: {len(changed_dirs)}")
          print(f"   Files uploaded: {total_files_uploaded}")
          print(f"   Files skipped: {total_files_skipped}")
          print(f"   Files failed: {total_files_failed}")
          print(f"   Total cost: ${total_cost:.4f}")
          print(f"{'='*60}")

          with open('/tmp/sync_summary.txt', 'w') as f:
              f.write(f"synced_count={len(changed_dirs)}\n")
              f.write(f"total_cost={total_cost:.4f}\n")
              f.write(f"files_uploaded={total_files_uploaded}\n")
              f.write(f"files_skipped={total_files_skipped}\n")
              f.write(f"files_failed={total_files_failed}\n")

          write_output("synced_count", len(changed_dirs))

          # Exit with error if failure rate is too high
          if total_files_uploaded + total_files_failed > 0:
              failure_rate = total_files_failed / (total_files_uploaded + total_files_failed)
              if failure_rate > 0.2:  # More than 20% failure rate
                  print(f"\n‚ùå High failure rate: {failure_rate:.1%}")
                  exit(1)
          EOF

      - name: Record last synced docs commit (state file in search-context)
        if: steps.sync.outcome == 'success' && steps.sync.outputs.synced_count != '0'
        shell: bash
        run: |
          set -euo pipefail
          DOCS_SHA=$(cd docs && git rev-parse HEAD)
          mkdir -p search-context/.context-sync
          echo "$DOCS_SHA" > search-context/.context-sync/last-docs-commit.txt
          cd search-context
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          if git diff --quiet .context-sync/last-docs-commit.txt; then
            echo "State file unchanged; no commit needed."
          else
            git add .context-sync/last-docs-commit.txt
            git commit -m "chore: update last synced docs commit to $DOCS_SHA"
            git push origin HEAD:main
            echo "Recorded and pushed docs commit $DOCS_SHA"
          fi

      - name: Report sync status
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -f /tmp/sync_summary.txt ]; then
            source /tmp/sync_summary.txt
            echo "üìä Sync Summary:"
            echo "   Directories synced: $synced_count"
            echo "   Files uploaded: ${files_uploaded:-0}"
            echo "   Files skipped: ${files_skipped:-0}"
            echo "   Files failed: ${files_failed:-0}"
            echo "   Total cost: \$$total_cost"
            echo "   State file path: search-context/.context-sync/last-docs-commit.txt"
            echo "   Next scheduled sync: Tomorrow at 2 AM UTC"

            # Show warning if there were failures
            if [ "${files_failed:-0}" -gt 0 ]; then
              echo ""
              echo "‚ö†Ô∏è  Warning: $files_failed files failed to upload"
            fi
          else
            echo "‚ö†Ô∏è Sync did not complete successfully"
          fi
