name: Sync Context Search Stores

on:
  # Run daily at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Allow manual trigger for testing or immediate updates
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-stores:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout search-context repository (for workflow)
        uses: actions/checkout@v4
        with:
          path: search-context
          fetch-depth: 0  # Need full history for diff since last sync

      - name: Checkout ain3sh/docs repository (for indexing)
        uses: actions/checkout@v4
        with:
          repository: ain3sh/docs
          path: docs
          fetch-depth: 0  # Need full history for change detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install google-genai python-dotenv

      - name: Get last sync timestamp
        id: last-sync
        run: |
          cd docs
          # Check if sync tag exists
          if git tag -l "last-context-sync" | grep -q "last-context-sync"; then
            LAST_SYNC_COMMIT=$(git rev-list -n 1 last-context-sync)
            echo "last_commit=$LAST_SYNC_COMMIT" >> $GITHUB_OUTPUT
            echo "Last sync commit: $LAST_SYNC_COMMIT"
          else
            # First run - sync everything
            echo "last_commit=" >> $GITHUB_OUTPUT
            echo "No previous sync found - will sync all directories"
          fi

      - name: Detect changed directories since last sync
        id: changed-dirs
        env:
          LAST_SYNC_COMMIT: ${{ steps.last-sync.outputs.last_commit }}
        run: |
          cd docs
          CHANGED_DIRS=$(python - <<'EOF'
          import os
          import sys
          import json
          from pathlib import Path

          def get_indexable_stores(docs_path, max_depth=2):
              """
              Find all indexable documentation directories.
              Only includes directories with files at their root level.
              Skips empty namespace containers.
              """
              indexable = []

              def scan_directory(path, depth=0, prefix=""):
                  if depth > max_depth:
                      return

                  # Count files at this directory's root (non-recursive)
                  files_at_root = sum(1 for f in path.iterdir()
                                    if f.is_file() and not f.name.startswith('.'))

                  if files_at_root > 0:
                      # This directory has documentation files - index it
                      store_name = f"{prefix}{path.name}" if prefix else path.name
                      indexable.append(store_name)
                      print(f"  ‚úÖ Indexable: {store_name} ({files_at_root} files at root)", file=sys.stderr)
                  else:
                      # No files at root - check subdirectories (it's a namespace container)
                      print(f"  ‚è≠Ô∏è  Skipping: {prefix}{path.name} (0 files at root, checking subdirs)", file=sys.stderr)
                      subdirs = [d for d in path.iterdir()
                               if d.is_dir() and not d.name.startswith('.')]

                      for subdir in subdirs:
                          new_prefix = f"{prefix}{path.name}/" if prefix or depth > 0 else f"{path.name}/"
                          scan_directory(subdir, depth + 1, new_prefix)

              # Scan all top-level directories
              for item in Path(docs_path).iterdir():
                  if item.is_dir() and not item.name.startswith('.'):
                      scan_directory(item, depth=0)

              return indexable

          # Check if this is first sync or incremental
          last_commit = os.environ.get('LAST_SYNC_COMMIT', '')

          if not last_commit:
              # First sync - get all indexable stores
              print("First sync - scanning all directories:", file=sys.stderr)
              all_stores = get_indexable_stores('.')
              print(f"\nFound {len(all_stores)} indexable stores: {all_stores}", file=sys.stderr)
              print(json.dumps(all_stores))
          else:
              # Incremental sync - get changed files, then filter to indexable stores
              import subprocess
              result = subprocess.run(
                  ['git', 'diff', '--name-only', last_commit, 'HEAD'],
                  capture_output=True,
                  text=True
              )
              changed_files = result.stdout.strip().split('\n')

              if not changed_files or changed_files == ['']:
                  print("No changes since last sync", file=sys.stderr)
                  print(json.dumps([]))
              else:
                  # Extract directories from changed files (up to 2 levels)
                  changed_paths = set()
                  for file in changed_files:
                      if file.startswith('.github/'):
                          continue
                      parts = file.split('/')
                      if len(parts) >= 1:
                          changed_paths.add(parts[0])
                      if len(parts) >= 2:
                          changed_paths.add(f"{parts[0]}/{parts[1]}")

                  # Filter to only indexable stores
                  all_stores = get_indexable_stores('.')
                  changed_stores = [s for s in all_stores if s in changed_paths]

                  print(f"\nChanged files in {len(changed_paths)} paths", file=sys.stderr)
                  print(f"Filtered to {len(changed_stores)} indexable stores: {changed_stores}", file=sys.stderr)
                  print(json.dumps(changed_stores))
          EOF
          )
          {
            printf '%s\n' "changed_dirs<<EOF_CHANGED_DIRS"
            printf '%s\n' "$CHANGED_DIRS"
            printf '%s\n' "EOF_CHANGED_DIRS"
          } >> $GITHUB_OUTPUT

      - name: Sync FileSearchStores
        id: sync
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CHANGED_DIRS: ${{ steps.changed-dirs.outputs.changed_dirs }}
        run: |
          python - <<'EOF'
          import os
          import json
          import time
          from pathlib import Path
          from google import genai
          from google.genai import types

          # Initialize client
          client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])

          # Get changed directories
          changed_dirs = json.loads(os.environ.get('CHANGED_DIRS', '[]'))

          if not changed_dirs:
              print("No directories changed, skipping sync")
              exit(0)

          total_cost = 0.0

          for store_name in changed_dirs:
              print(f"\n{'='*60}")
              print(f"Processing store: {store_name}")
              print(f"{'='*60}")

              # Delete existing store if it exists
              try:
                  stores = list(client.file_search_stores.list())
                  existing = [s for s in stores if s.display_name == store_name]
                  if existing:
                      print(f"Deleting existing store: {existing[0].name}")
                      client.file_search_stores.delete(
                          name=existing[0].name,
                          force=True  # ‚úÖ FIXED: Direct parameter, not in config
                      )
                      time.sleep(2)  # Wait for deletion to propagate
              except Exception as e:
                  print(f"‚ùå FATAL: Error deleting store {store_name}: {e}")
                  print(f"   Skipping this store to avoid duplicate creation")
                  continue

              # Create new store
              print(f"Creating new FileSearchStore: {store_name}")
              store = client.file_search_stores.create(
                  config={'display_name': store_name}
              )

              # Upload all files in the directory from docs repo
              store_path = Path('docs') / store_name
              files = list(store_path.rglob('*.md')) + \
                      list(store_path.rglob('*.txt')) + \
                      list(store_path.rglob('*.py')) + \
                      list(store_path.rglob('*.js')) + \
                      list(store_path.rglob('*.json'))

              print(f"Found {len(files)} files to upload")

              operations = []
              total_tokens = 0

              for file_path in files:
                  try:
                      # Calculate tokens (rough estimate: 1 token ‚âà 4 chars)
                      file_size = file_path.stat().st_size
                      estimated_tokens = file_size // 4
                      total_tokens += estimated_tokens

                      print(f"Uploading: {file_path}")
                      op = client.file_search_stores.upload_to_file_search_store(
                          file=str(file_path),
                          file_search_store_name=store.name,
                          config={
                              'display_name': str(file_path.relative_to(store_path))
                          }
                      )
                      operations.append(op)
                  except Exception as e:
                      print(f"Error uploading {file_path}: {e}")

              # Wait for all uploads to complete
              print("Waiting for uploads to complete...")
              for i, op in enumerate(operations):
                  max_retries = 120  # 10 minutes max (120 * 5 seconds)
                  retry_count = 0
                  while not op.done:
                      if retry_count >= max_retries:
                          raise TimeoutError(f"Upload operation timed out after {max_retries * 5} seconds")
                      time.sleep(5)
                      op = client.operations.get(op)
                      retry_count += 1
                  print(f"  Upload {i+1}/{len(operations)} complete")

              # Calculate cost
              cost = (total_tokens / 1_000_000) * 0.15
              total_cost += cost

              print(f"\nStore '{store_name}' sync complete:")
              print(f"  - Files indexed: {len(files)}")
              print(f"  - Estimated tokens: {total_tokens:,}")
              print(f"  - Cost: ${cost:.4f}")

          print(f"\n{'='*60}")
          print(f"Total sync cost: ${total_cost:.4f}")
          print(f"Directories synced: {len(changed_dirs)}")
          print(f"{'='*60}")

          # Write summary for next step
          with open('/tmp/sync_summary.txt', 'w') as f:
              f.write(f"synced_count={len(changed_dirs)}\n")
              f.write(f"total_cost={total_cost:.4f}\n")
          EOF

      - name: Tag last sync commit in docs repo
        if: steps.sync.outcome == 'success'
        run: |
          cd docs
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Delete old tag if exists
          git tag -d last-context-sync || true
          git push origin :refs/tags/last-context-sync || true

          # Create new tag at current commit
          git tag -a last-context-sync -m "Last context search sync: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          git push origin last-context-sync

          DOCS_SHA=$(git rev-parse HEAD)
          echo "‚úÖ Tagged docs commit $DOCS_SHA as last successful sync"

      - name: Report sync status
        if: always()
        run: |
          if [ -f /tmp/sync_summary.txt ]; then
            source /tmp/sync_summary.txt
            echo "üìä Sync Summary:"
            echo "   Directories synced: $synced_count"
            echo "   Total cost: \$$total_cost"
            echo "   Next sync: Tomorrow at 2 AM UTC"
          else
            echo "‚ö†Ô∏è Sync did not complete successfully"
          fi
