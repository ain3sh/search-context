name: Sync Context Search Stores

on:
  # Run daily at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Allow manual trigger for testing or immediate updates
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-stores:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout search-context repository (for workflow & state file)
        uses: actions/checkout@v4
        with:
          path: search-context
          fetch-depth: 0

      - name: Checkout ain3sh/docs repository (for indexing)
        uses: actions/checkout@v4
        with:
          repository: ain3sh/docs
          path: docs
          fetch-depth: 0  # Need full history for change detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install google-genai python-dotenv

      - name: Get last sync timestamp (from state file in search-context)
        id: last-sync
        shell: bash
        run: |
          set -euo pipefail
          STATE_FILE="search-context/.context-sync/last-docs-commit.txt"
          if [ -f "$STATE_FILE" ]; then
            LAST_SYNC_COMMIT=$(cat "$STATE_FILE")
            echo "Found previous sync marker: $LAST_SYNC_COMMIT"
            echo "last_commit=$LAST_SYNC_COMMIT" > $GITHUB_OUTPUT
          else
            echo "No previous sync marker file found - full sync will run."
            echo "last_commit=" > $GITHUB_OUTPUT
          fi

      - name: Detect changed directories since last sync
        id: changed-dirs
        env:
          LAST_SYNC_COMMIT: ${{ steps.last-sync.outputs.last_commit }}
        shell: bash
        run: |
          set -euo pipefail
          cd docs
          CHANGED_DIRS=$(python - <<'EOF'
          import os
          import sys
          import json
          from pathlib import Path

          def get_indexable_stores(docs_path, max_depth=2):
              """
              Find all indexable documentation directories.
              Only includes directories with files at their root level.
              Skips empty namespace containers.
              """
              indexable = []

              def scan_directory(path, depth=0, prefix=""):
                  if depth > max_depth:
                      return

                  files_at_root = sum(
                      1 for f in path.iterdir()
                      if f.is_file() and not f.name.startswith('.')
                  )

                  if files_at_root > 0:
                      store_name = f"{prefix}{path.name}" if prefix else path.name
                      indexable.append(store_name)
                      print(f"  ‚úÖ Indexable: {store_name} ({files_at_root} files at root)", file=sys.stderr)
                  else:
                      print(f"  ‚è≠Ô∏è  Skipping: {prefix}{path.name} (0 files at root, checking subdirs)", file=sys.stderr)
                      subdirs = [
                          d for d in path.iterdir()
                          if d.is_dir() and not d.name.startswith('.')
                      ]
                      for subdir in subdirs:
                          new_prefix = f"{prefix}{path.name}/" if prefix or depth > 0 else f"{path.name}/"
                          scan_directory(subdir, depth + 1, new_prefix)

              for item in Path(docs_path).iterdir():
                  if item.is_dir() and not item.name.startswith('.'):
                      scan_directory(item, depth=0)

              return indexable

          last_commit = os.environ.get('LAST_SYNC_COMMIT', '')

          if not last_commit:
              print("First (or full) sync - scanning all directories", file=sys.stderr)
              all_stores = get_indexable_stores('.')
              print(json.dumps(all_stores))
          else:
              import subprocess
              result = subprocess.run(
                  ['git', 'diff', '--name-only', last_commit, 'HEAD'],
                  capture_output=True,
                  text=True
              )
              changed_files = result.stdout.strip().split('\n')

              if not changed_files or changed_files == ['']:
                  print("No changes since last sync", file=sys.stderr)
                  print(json.dumps([]))
              else:
                  changed_paths = set()
                  for file in changed_files:
                      if file.startswith('.github/'):
                          continue
                      parts = file.split('/')
                      if parts:
                          changed_paths.add(parts[0])
                      if len(parts) >= 2:
                          changed_paths.add(f"{parts[0]}/{parts[1]}")

                  all_stores = get_indexable_stores('.')
                  changed_stores = [s for s in all_stores if s in changed_paths]

                  print(f"Changed paths considered: {len(changed_paths)}", file=sys.stderr)
                  print(f"Filtered to indexable changed stores: {changed_stores}", file=sys.stderr)
                  print(json.dumps(changed_stores))
          EOF
          )
          {
            printf '%s\n' "changed_dirs<<EOF_CHANGED_DIRS"
            printf '%s\n' "$CHANGED_DIRS"
            printf '%s\n' "EOF_CHANGED_DIRS"
          } > $GITHUB_OUTPUT

      - name: Sync FileSearchStores
        id: sync
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CHANGED_DIRS: ${{ steps.changed-dirs.outputs.changed_dirs }}
        shell: bash
        run: |
          set -euo pipefail
          python - <<'EOF'
          import os
          import json
          import time
          from pathlib import Path
          from google import genai
          from concurrent.futures import ThreadPoolExecutor, as_completed

          client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
          changed_dirs = json.loads(os.environ.get('CHANGED_DIRS', '[]'))

          def write_output(key, value):
              out = os.environ.get('GITHUB_OUTPUT')
              if out:
                  with open(out, 'a') as f:
                      f.write(f"{key}={value}\n")

          if not changed_dirs:
              print("No directories changed, skipping sync.")
              write_output("synced_count", 0)
              with open('/tmp/sync_summary.txt', 'w') as f:
                  f.write("synced_count=0\n")
                  f.write("total_cost=0.0000\n")
              exit(0)

          total_cost = 0.0

          for store_name in changed_dirs:
              print(f"\n{'='*60}")
              print(f"Processing store: {store_name}")
              print(f"{'='*60}")

              # Delete existing store if present
              try:
                  stores = list(client.file_search_stores.list())
                  existing = [s for s in stores if s.display_name == store_name]
                  if existing:
                      print(f"Deleting existing store: {existing[0].name}")
                      client.file_search_stores.delete(
                          name=existing[0].name,
                          config={'force': True}
                      )
                      time.sleep(2)
              except Exception as e:
                  print(f"‚ùå Error deleting store {store_name}: {e}")
                  print("   Continuing with creation attempt.")

              # Create new store
              print(f"Creating new FileSearchStore: {store_name}")
              store = client.file_search_stores.create(config={'display_name': store_name})

              # OPTIMIZATION: Use os.walk for robust file discovery
              store_path = Path('docs') / store_name
              extensions = {'.md', '.mdx', '.txt', '.py', '.js', '.json'}
              files = []
              
              for root, _, filenames in os.walk(store_path):
                  for filename in filenames:
                      if Path(filename).suffix.lower() in extensions:
                          files.append(Path(root) / filename)

              print(f"Found {len(files)} files to upload.")
              
              if len(files) == 0:
                  print("‚ö†Ô∏è  No files found, skipping store.")
                  continue

              # OPTIMIZATION: Parallel upload with ThreadPoolExecutor
              operations = []
              total_tokens = 0
              
              def upload_file(file_path):
                  """Upload a single file and return operation"""
                  try:
                      file_size = file_path.stat().st_size
                      estimated_tokens = file_size // 4
                      
                      op = client.file_search_stores.upload_to_file_search_store(
                          file=str(file_path),
                          file_search_store_name=store.name,
                          config={'display_name': str(file_path.relative_to(store_path))}
                      )
                      return (op, estimated_tokens, file_path, None)
                  except Exception as e:
                      return (None, 0, file_path, str(e))
              
              # Upload files in parallel (max 10 concurrent)
              print("Uploading files in parallel...")
              with ThreadPoolExecutor(max_workers=10) as executor:
                  futures = {executor.submit(upload_file, f): f for f in files}
                  
                  for i, future in enumerate(as_completed(futures), 1):
                      op, tokens, file_path, error = future.result()
                      if error:
                          print(f"  ‚ùå [{i}/{len(files)}] Error: {file_path.name}: {error}")
                      else:
                          operations.append(op)
                          total_tokens += tokens
                          if i % 10 == 0 or i == len(files):
                              print(f"  ‚úÖ [{i}/{len(files)}] queued")

              # OPTIMIZATION: Batch polling with reduced sleep
              print(f"\nWaiting for {len(operations)} uploads to complete...")
              completed = 0
              max_wait = 600  # 10 minutes max
              start_time = time.time()
              last_report = 0
              
              while completed < len(operations):
                  if time.time() - start_time > max_wait:
                      print(f"‚ö†Ô∏è  Timeout: {completed}/{len(operations)} completed")
                      break
                  
                  # Check all operations in batch
                  newly_completed = 0
                  for i, op in enumerate(operations):
                      if not op.done:
                          operations[i] = client.operations.get(op)
                          if operations[i].done:
                              newly_completed += 1
                  
                  completed += newly_completed
                  
                  # Report progress every 10 completions or when done
                  if newly_completed > 0 and (completed - last_report >= 10 or completed == len(operations)):
                      print(f"  [{completed}/{len(operations)}] completed")
                      last_report = completed
                  
                  if completed < len(operations):
                      time.sleep(2)  # Poll every 2s instead of 5s

              cost = (total_tokens / 1_000_000) * 0.15
              total_cost += cost

              print(f"\n‚úÖ Store '{store_name}' sync complete:")
              print(f"   Files indexed: {len(operations)}")
              print(f"   Estimated tokens: {total_tokens:,}")
              print(f"   Cost: ${cost:.4f}")

          print(f"\n{'='*60}")
          print(f"‚úÖ Total sync cost: ${total_cost:.4f}")
          print(f"   Directories synced: {len(changed_dirs)}")
          print(f"{'='*60}")

          with open('/tmp/sync_summary.txt', 'w') as f:
              f.write(f"synced_count={len(changed_dirs)}\n")
              f.write(f"total_cost={total_cost:.4f}\n")

          write_output("synced_count", len(changed_dirs))
          EOF

      - name: Record last synced docs commit (state file in search-context)
        if: steps.sync.outcome == 'success' && steps.sync.outputs.synced_count != '0'
        shell: bash
        run: |
          set -euo pipefail
          DOCS_SHA=$(cd docs && git rev-parse HEAD)
          mkdir -p search-context/.context-sync
          echo "$DOCS_SHA" > search-context/.context-sync/last-docs-commit.txt
          cd search-context
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          if git diff --quiet .context-sync/last-docs-commit.txt; then
            echo "State file unchanged; no commit needed."
          else
            git add .context-sync/last-docs-commit.txt
            git commit -m "chore: update last synced docs commit to $DOCS_SHA"
            git push origin HEAD:main
            echo "Recorded and pushed docs commit $DOCS_SHA"
          fi

      - name: Report sync status
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -f /tmp/sync_summary.txt ]; then
            source /tmp/sync_summary.txt
            echo "üìä Sync Summary:"
            echo "   Directories synced: $synced_count"
            echo "   Total cost: \$$total_cost"
            echo "   State file path: search-context/.context-sync/last-docs-commit.txt"
            echo "   Next scheduled sync: Tomorrow at 2 AM UTC"
          else
            echo "‚ö†Ô∏è Sync did not complete successfully"
          fi
